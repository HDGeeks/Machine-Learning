{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRMtpuCVepJC"
      },
      "source": [
        "# **Data Representation - From words to numbers**\n",
        "\n",
        "**Term Document Matrix:**\n",
        "\n",
        "Document 1: \"The dog is a nice dog\"\n",
        "\n",
        "Document 2: \"The ant is no dog\"\n",
        "\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "```\n",
        "1. Assign each unique word(=type) from the corpus an index:\n",
        "      the -> 0\n",
        "      dog -> 1\n",
        "      is -> 2\n",
        "      a -> 3\n",
        "      nice -> 4\n",
        "      ant -> 5\n",
        "      no -> 6\n",
        "\n",
        "The set of unique words is referred to as the Vocabulary\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "2. Create a matrix filled with zeros with the following dimensions:\n",
        "      rows = number of documents\n",
        "      columns = size of the vocabulary\n",
        "\n",
        "Matrix:\n",
        "            the [0]  dog [1]  is [2]  a [3]  nice [4]  ant [5]  no [6]\n",
        "Doc 1  [0]       0        0       0      0         0        0        0\n",
        "Doc 2  [1]       0        0       0      0         0        0        0\n",
        "```\n",
        "\n",
        "```\n",
        "3. Iterate over the words in the documents and increment the values\n",
        "in the matrix at position (docIndex, wordindex) by one:\n",
        "\"the dog is a nice dog\"\n",
        "=>\n",
        "\n",
        "            the [0]  dog [1]  is [2]  a [3]  nice [4]  ant [5]  no [6]\n",
        "Doc 1  [0]       1        2       1      1         1        0        0\n",
        "Doc 2  [1]       0        0       0      0         0        0        0\n",
        "\n",
        "\n",
        "\"the ant is no dog\"\n",
        "=>\n",
        "\n",
        "            the [0]  dog [1]  is [2]  a [3]  nice [4]  ant [5]  no [6]\n",
        "Doc 1  [0]       1        2       1      1         1        0        0\n",
        "Doc 2  [1]       1        1       1      0         0        1        1\n",
        "```\n",
        "\n",
        "The resulting matrix is a so-called \"term-document\" matrix, where one row represents a document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVAl_ISaeeeU",
        "outputId": "3f401a92-f2cd-4065-c686-fc81e28bd14c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def getDocMatrix():\n",
        "    ###the documents are represented as a nested list (steps like tokenization etc. are omitted for simplicity)\n",
        "\n",
        "    documents = [[\"the\",\"dog\",\"is\",\"a\",\"nice\",\"dog\"],[\"the\",\"ant\",\"is\",\"no\",\"dog\"]]\n",
        "\n",
        "    #determine the amount of documents in documents\n",
        "    nrDocuments = len(documents)\n",
        "\n",
        "    #vocabulary = set(sum(documents,[])) #simple, but implicit\n",
        "    ###explicity determining the vocabulary:\n",
        "\n",
        "    #init empty vocab\n",
        "    vocabulary = []\n",
        "\n",
        "    #iterate over documents\n",
        "    for document in documents:\n",
        "      #iterate over words in document\n",
        "      for token in document:\n",
        "        #check if a token is in vocabulary - if not, add to vocab\n",
        "        if not token in vocabulary:\n",
        "          vocabulary.append(token)\n",
        "    #print the vocabulary as list\n",
        "    print (vocabulary)\n",
        "\n",
        "    #determine the size of the vocabulary\n",
        "    vocSize = len(vocabulary)\n",
        "    print (vocSize)\n",
        "\n",
        "    #create a range of indices needed (i.e. the numbers 0-6)\n",
        "    indices = range(len(vocabulary))\n",
        "    print (indices)\n",
        "\n",
        "    #create a dictionary out of the vocabulary and the range of indices\n",
        "    vocabularyIndex = dict(zip(vocabulary, indices))\n",
        "    print (vocabularyIndex)\n",
        "\n",
        "    #initialize the term document matrix with zeros\n",
        "    docMatrix = torch.zeros((nrDocuments,vocSize))\n",
        "\n",
        "    for i, document in enumerate(documents):\n",
        "        print (f\"\\nProcessing document {i}: and the content is {document}\")\n",
        "\n",
        "        #replace the words in the current document by their respective indices and transform the list to a tensor\n",
        "        tmp = torch.tensor(list(map(lambda x: vocabularyIndex[x], document)))\n",
        "        print (f\"\\nDocument {i} as type indices from the vocabularyIndex: \")\n",
        "        print (tmp) #document with words replaced by indices\n",
        "        print(f\"The shape of the tmp tensor is: {tmp.shape} and as dimensions: {tmp.dim()}\")\n",
        "\n",
        "        # Basically we create a zero matrix with rows size of tokens of document and columns size of vocabulary\n",
        "        # Then we set the positions of the tokens to 1\n",
        "        # Very sparse representation of tokens in document\n",
        "\n",
        "        # define the rows and columns\n",
        "        rows , cols = tmp.shape[0], len(vocabulary)\n",
        "        oneHot = torch.zeros((rows,cols)) #intermediate matrix where each row represents one token\n",
        "\n",
        "        rows = torch.arange(tmp.shape[0])\n",
        "        print (f\"\\nDocument {i} row indices (tokens):\")\n",
        "        print (rows)\n",
        "        oneHot[rows,tmp] = 1 # Basically for all tokens (rows) we set the respective column (tmp) to 1\n",
        "\n",
        "        print (f\"\\nDocument {i} will have {rows.shape[0]} rows and {cols} columns in the one-hot encoded matrix.\")\n",
        "        print (oneHot)\n",
        "        print (f\"The shape of the oneHot tensor is: {oneHot.shape} and as dimensions: {oneHot.dim()}\")\n",
        "\n",
        "\n",
        "        # Sum up the one-hot encoded vectors to get the document vector basically add up all rows as vectors\n",
        "        docVector = torch.sum(oneHot,0) # zero means add rows wise as axis=0 in numpy\n",
        "        print (f\"\\nThe document vector for document {i} is:\")\n",
        "        print (docVector)\n",
        "\n",
        "        #assign the document vector to the respective row in the document matrix\n",
        "        docMatrix[i] = docVector\n",
        "    print (\"\\n Finally , This is the term-document matrix:\")\n",
        "    return (docMatrix)\n",
        "docMatrix = getDocMatrix()\n",
        "print (docMatrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfaWc9V4pBrb"
      },
      "source": [
        "TF-IDF: Taking into account rare words and text length\n",
        "\n",
        "Side effect: Normalization and \"better\" value range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsVpLD2JpMSl",
        "outputId": "9bb06f3b-8131-413d-9128-f745c32320d2"
      },
      "outputs": [],
      "source": [
        "print (\"Term-Document Matrix\")\n",
        "print (docMatrix)\n",
        "\n",
        "# Word frequency for whole corpus:\n",
        "wf = torch.sum(docMatrix,0) # sum over rows (documents)\n",
        "print (\"\\nHow often does a token occur in the corpus:\")\n",
        "print (wf)\n",
        "\n",
        "# Nr. of words per document\n",
        "wc = torch.sum(docMatrix,1) # sum over columns (tokens) per document\n",
        "print (\"\\nHow many words are in the respective documents:\")\n",
        "print (wc)\n",
        "\n",
        "# term frequency\n",
        "tf = torch.div(docMatrix.transpose(0,1),wc)\n",
        "tf = tf.transpose(0,1)\n",
        "print (\"\\nNormalise token frequency by document length:\")\n",
        "print (tf)\n",
        "\n",
        "# document frequency\n",
        "# This counts non zero entries per column (token) in the docMatrix\n",
        "# so it will have dim of (1,voacSize)\n",
        "df = torch.count_nonzero(docMatrix,0)\n",
        "print (\"\\nIn how many documents of the corpus does a token occur:\")\n",
        "print (df)\n",
        "\n",
        "# inverse document frequency\n",
        "# Get dimensions or size of docMatrix\n",
        "print (docMatrix.shape, docMatrix.shape[0], docMatrix.dim())\n",
        "idf = docMatrix.shape[0]/df+1 #+1 to avoid division by zero errors\n",
        "print (\"\\nInverse of the document frequency:\")\n",
        "print (idf)\n",
        "#log idf\n",
        "logIdf = torch.log(idf) #taking the logarithm\n",
        "print (\"\\nLog inverse document frequency: (specificity of a term)\")\n",
        "print (logIdf)\n",
        "\n",
        "#tf-idf\n",
        "tfidf = tf*logIdf\n",
        "print (\"\\nTF-IDF - Term Frequency * Term Specificity\")\n",
        "print (tfidf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
